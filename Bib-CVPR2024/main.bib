@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@misc{Authors14,
 author = {FirstName LastName},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {FirstName LastName},
 title = {Frobnication tutorial},
 note = {Supplied as supplemental material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = PAMI,
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@article{Alpher04,
author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}

@inproceedings{Alpher05,
author = {FirstName Alpher and FirstName Gamow},
title = {Can a computer frobnicate?},
booktitle = CVPR,
pages = {234--778},
year = 2005
}

@INPROCEEDINGS{abandoned-cart,
  author={Melegrito, Mark P. and Alon, Alvin Sarraga and Militante, Sammy V. and Austria, Yolanda D. and Polinar, Myriam J. and Mirabueno, Maria Concepcion A.},
  booktitle={2021 IEEE International Conference on Artificial Intelligence in Engineering and Technology (IICAIET)}, 
  title={Abandoned-Cart-Vision: Abandoned Cart Detection Using a Deep Object Detection Approach in a Shopping Parking Space}, 
  year={2021},
  volume={},
  number={},
  pages={1-5},
  keywords={Training;Space vehicles;Conferences;Transfer learning;Object detection;Video surveillance;Distance measurement;shopping cart;deep learning;object detection;YOLOv3;abandoned shopping cart},
  doi={10.1109/IICAIET51634.2021.9573963}}

@INPROCEEDINGS{detection-identifcation,
  author={Jadhav, Lakhan H. and Momin, Bashirahamad F.},
  booktitle={2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)}, 
  title={Detection and identification of unattended/removed objects in video surveillance}, 
  year={2016},
  volume={},
  number={},
  pages={1770-1773},
  keywords={Object detection;Object recognition;Feature extraction;Conferences;Video surveillance;Cameras;video surveillance;background modeling;abandoned object},
  doi={10.1109/RTEICT.2016.7808138}}

@article{wagner,
 ISSN = {03441369},
 URL = {http://www.jstor.org/stable/26426729},
 abstract = {This study reports on a new procedure for tracking the in-store behavior of customers, observed in two grocery stores located in a metropolitan area of a capital city in Europe. Shopping paths were gathered utilizing disguised human observations recorded on a tablet computer, which enabled the collection of both customer purchases and the locations where the shoppers stopped or parked their carts. The two stores differed with respect to their rather traditional and more modern layouts. This research focuses on the impact of store design on patrons’ shopping cart parking behavior. Customers navigated more freely and conveniently in the modern store. Essentially, it was found that shopping cart parking behavior mediates the relationship between in-store movement and purchase behavior. In particular, shoppers parking carts during their store visit tended to buy more. This effect was more pronounced in the modern, free-flow store layout. In addition, the study presents the typical patterns of shopping behavior in different store areas. The findings are relevant from both a managerial and a methodological perspective.},
 author = {Udo Wagner and Claus Ebster and Ulrike Eske and Wolfgang Weitzl},
 journal = {Marketing: ZFP – Journal of Research and Management},
 number = {3},
 pages = {165--175},
 publisher = {Verlag C.H.Beck},
 title = {The Influence of Shopping Carts on Customer Behavior in Grocery Stores},
 urldate = {2024-05-06},
 volume = {36},
 year = {2014}
}

@misc{kornilov-dataset,
title = { shopping trolley Dataset },
type = { Open Source Dataset },
author = { Kirill Kornilov },
howpublished = { \url{ https://universe.roboflow.com/kirill-kornilov-kn3yx/shopping-trolley-kn5tj } },
url = { https://universe.roboflow.com/kirill-kornilov-kn3yx/shopping-trolley-kn5tj },
journal = { Roboflow Universe },
publisher = { Roboflow },
year = { 2023 },
month = { dec },
note = { visited on 2024-05-06 },
}

@misc{bakkal_dataset,
title = { Shopping Cart Dataset },
type = { Open Source Dataset },
author = { Furkan Bakkal },
howpublished = { \url{ https://universe.roboflow.com/furkan-bakkal/shopping-cart-1r48s } },
url = { https://universe.roboflow.com/furkan-bakkal/shopping-cart-1r48s },
journal = { Roboflow Universe },
publisher = { Roboflow },
year = { 2021 },
month = { nov },
note = { visited on 2024-05-06 },
}


@Article{molecules,
AUTHOR = {Rácz, Anita and Bajusz, Dávid and Héberger, Károly},
TITLE = {Effect of Dataset Size and Train/Test Split Ratios in QSAR/QSPR Multiclass Classification},
JOURNAL = {Molecules},
VOLUME = {26},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {1111},
URL = {https://www.mdpi.com/1420-3049/26/4/1111},
PubMedID = {33669834},
ISSN = {1420-3049},
ABSTRACT = {Applied datasets can vary from a few hundred to thousands of samples in typical quantitative structure-activity/property (QSAR/QSPR) relationships and classification. However, the size of the datasets and the train/test split ratios can greatly affect the outcome of the models, and thus the classification performance itself. We compared several combinations of dataset sizes and split ratios with five different machine learning algorithms to find the differences or similarities and to select the best parameter settings in nonbinary (multiclass) classification. It is also known that the models are ranked differently according to the performance merit(s) used. Here, 25 performance parameters were calculated for each model, then factorial ANOVA was applied to compare the results. The results clearly show the differences not just between the applied machine learning algorithms but also between the dataset sizes and to a lesser extent the train/test split ratios. The XGBoost algorithm could outperform the others, even in multiclass modeling. The performance parameters reacted differently to the change of the sample set size; some of them were much more sensitive to this factor than the others. Moreover, significant differences could be detected between train/test split ratios as well, exerting a great effect on the test validation of our models.},
DOI = {10.3390/molecules26041111}
}

@Article{robotic-arm,
AUTHOR = {Hernandez, Jaime and Sunny, Md Samiul Haque and Sanjuan, Javier and Rulik, Ivan and Zarif, Md Ishrak Islam and Ahamed, Sheikh Iqbal and Ahmed, Helal Uddin and Rahman, Mohammad H},
TITLE = {Current Designs of Robotic Arm Grippers: A Comprehensive Systematic Review},
JOURNAL = {Robotics},
VOLUME = {12},
YEAR = {2023},
NUMBER = {1},
ARTICLE-NUMBER = {5},
URL = {https://www.mdpi.com/2218-6581/12/1/5},
ISSN = {2218-6581},
ABSTRACT = {Recent technological advances enable gripper-equipped robots to perform many tasks traditionally associated with the human hand, allowing the use of grippers in a wide range of applications. Depending on the application, an ideal gripper design should be affordable, energy-efficient, and adaptable to many situations. However, regardless of the number of grippers available on the market, there are still many tasks that are difficult for grippers to perform, which indicates the demand and room for new designs to compete with the human hand. Thus, this paper provides a comprehensive review of robotic arm grippers to identify the benefits and drawbacks of various gripper designs. The research compares gripper designs by considering the actuation mechanism, degrees of freedom, grasping capabilities with multiple objects, and applications, concluding which should be the gripper design with the broader set of capabilities.},
DOI = {10.3390/robotics12010005}
}

@Article{recent-developments,
AUTHOR = {Yu, Xiaoyan and Marinov, Marin},
TITLE = {A Study on Recent Developments and Issues with Obstacle Detection Systems for Automated Vehicles},
JOURNAL = {Sustainability},
VOLUME = {12},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {3281},
URL = {https://www.mdpi.com/2071-1050/12/8/3281},
ISSN = {2071-1050},
ABSTRACT = {This paper reviews current developments and discusses some critical issues with obstacle detection systems for automated vehicles. The concept of autonomous driving is the driver towards future mobility. Obstacle detection systems play a crucial role in implementing and deploying autonomous driving on our roads and city streets. The current review looks at technology and existing systems for obstacle detection. Specifically, we look at the performance of LIDAR, RADAR, vision cameras, ultrasonic sensors, and IR and review their capabilities and behaviour in a number of different situations: during daytime, at night, in extreme weather conditions, in urban areas, in the presence of smooths surfaces, in situations where emergency service vehicles need to be detected and recognised, and in situations where potholes need to be observed and measured. It is suggested that combining different technologies for obstacle detection gives a more accurate representation of the driving environment. In particular, when looking at technological solutions for obstacle detection in extreme weather conditions (rain, snow, fog), and in some specific situations in urban areas (shadows, reflections, potholes, insufficient illumination), although already quite advanced, the current developments appear to be not sophisticated enough to guarantee 100% precision and accuracy, hence further valiant effort is needed.},
DOI = {10.3390/su12083281}
}

@article{mobile-robot-navigation,
author = {Pandey, Dr. Anish},
year = {2017},
month = {05},
pages = {1-12},
title = {Mobile Robot Navigation and Obstacle Avoidance Techniques: A Review},
volume = {2},
journal = {International Robotics & Automation Journal},
doi = {10.15406/iratj.2017.02.00023}
}

@misc{pytorch,
title = {{PyTorch documentation}},
howpublished = {\url{https://pytorch.org/docs/stable/index.html}},
note = {Accessed: 2024-05-07},
year = 2024
}

@InProceedings{self-driving,
author="Devaki, P.
and Satish, Aakarsh
and Dixit, Shivam
and Urs, Snehal N.
and Kashyap, Tejasvi",
editor="Smys, S.
and Tavares, Jo{\~a}o Manuel R. S.
and Shi, Fuqian",
title="Self-driving Shopping Cart to Assist the Visually Impaired",
booktitle="Computational Vision and Bio-Inspired Computing",
year="2023",
publisher="Springer Nature Singapore",
address="Singapore",
pages="659--674",
abstract="As of now, there are no facilities that allow visually impaired people to go shopping independently, and they either have to depend on their friends and family to get things delivered to them. With the advent of many revolutionary technologies in the domain of automation, this research study proposes a novel idea to build a prototype by using Raspberry Pi and Arduino Uno leading to the development of a self-driving cart. Microcontrollers, motors, H-bridge drivers, and camera modules will be incorporated with different technologies such as computer vision, machine learning, and deep neural networks, which will all be integrated into a cart chassis that also holds a chair for visually impaired people. Microcontrollers such as Arduino Uno and Raspberry Pi are used to control the cart's motion and process the camera's live video output, respectively. Methods such as lane detection, object recognition, and obstacle detection are utilized for the cart to navigate and stop near the required products and avoid obstacles as it moves.",
isbn="978-981-19-9819-5"
}

@INPROCEEDINGS{intro-fuzzy,
  author={Dote, Y.},
  booktitle={Proceedings of IECON '95 - 21st Annual Conference on IEEE Industrial Electronics}, 
  title={Introduction to fuzzy logic}, 
  year={1995},
  volume={1},
  number={},
  pages={50-56 vol.1},
  keywords={Fuzzy logic;Fuzzy control;Control systems;Fuzzy systems;Humans;Neural networks;Industrial control;Computer networks;Electrical equipment industry;Computer industry},
  doi={10.1109/IECON.1995.483332}}

@article{accurate,
title = {Accurate bounding-box regression with distance-IoU loss for visual tracking},
journal = {Journal of Visual Communication and Image Representation},
volume = {83},
pages = {103428},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103428},
url = {https://www.sciencedirect.com/science/article/pii/S104732032100290X},
author = {Di Yuan and Xiu Shu and Nana Fan and Xiaojun Chang and Qiao Liu and Zhenyu He},
keywords = {Visual tracking, Bounding-box regression, Distance-IoU loss},
abstract = {Most existing trackers are based on using a classifier and multi-scale estimation to estimate the target state. Consequently, and as expected, trackers have become more stable while tracking accuracy has stagnated. While trackers adopt a maximum overlap method based on an intersection-over-union (IoU) loss to mitigate this problem, there are defects in the IoU loss itself, that make it impossible to continue to optimize the objective function when a given bounding box is completely contained within/without another bounding box; this makes it very challenging to accurately estimate the target state. Accordingly, in this paper, we address the above-mentioned problem by proposing a novel tracking method based on a distance-IoU (DIoU) loss, such that the proposed tracker consists of target estimation and target classification. The target estimation part is trained to predict the DIoU score between the target ground-truth bounding-box and the estimated bounding-box. The DIoU loss can maintain the advantage provided by the IoU loss while minimizing the distance between the center points of two bounding boxes, thereby making the target estimation more accurate. Moreover, we introduce a classification part that is trained online and optimized with a Conjugate-Gradient-based strategy to guarantee real-time tracking speed. Comprehensive experimental results demonstrate that the proposed method achieves competitive tracking accuracy when compared to state-of-the-art trackers while with a real-time tracking speed.}
}

@misc{openimagesv7,
  title        = {Open Images V7 Dataset},
  howpublished = {Ultralytics Documentation},
  year         = 2024,
  note         = {Available online: \url{https://docs.ultralytics.com/datasets/detect/open-images-v7/#sample-data-and-annotations}}
}

@misc{coco,
  title        = {COCO Dataset},
  howpublished = {Ultralytics Documentation},
  year         = 2024,
  note         = {Available online: https://docs.ultralytics.com/datasets/detect/coco/}}
}

@InProceedings{microsoft_coco,
author="Lin, Tsung-Yi
and Maire, Michael
and Belongie, Serge
and Hays, James
and Perona, Pietro
and Ramanan, Deva
and Doll{\'a}r, Piotr
and Zitnick, C. Lawrence",
editor="Fleet, David
and Pajdla, Tomas
and Schiele, Bernt
and Tuytelaars, Tinne",
title="Microsoft COCO: Common Objects in Context",
booktitle="Computer Vision -- ECCV 2014",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="740--755",
abstract="We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
isbn="978-3-319-10602-1"
}

@article{openimagesv4,
  author       = {Alina Kuznetsova and
                  Hassan Rom and
                  Neil Alldrin and
                  Jasper R. R. Uijlings and
                  Ivan Krasin and
                  Jordi Pont{-}Tuset and
                  Shahab Kamali and
                  Stefan Popov and
                  Matteo Malloci and
                  Tom Duerig and
                  Vittorio Ferrari},
  title        = {The Open Images Dataset {V4:} Unified image classification, object
                  detection, and visual relationship detection at scale},
  journal      = {CoRR},
  volume       = {abs/1811.00982},
  year         = {2018},
  url          = {http://arxiv.org/abs/1811.00982},
  eprinttype    = {arXiv},
  eprint       = {1811.00982},
  timestamp    = {Thu, 22 Nov 2018 17:58:30 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1811-00982.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{why_does,
  title = 	 {Why Does Unsupervised Pre-training Help Deep Learning?},
  author = 	 {Erhan, Dumitru and Courville, Aaron and Bengio, Yoshua and Vincent, Pascal},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {201--208},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/erhan10a/erhan10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/erhan10a.html},
  abstract = 	 {Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder   variants with impressive results being obtained in several areas, mostly   on vision and language datasets.  The best results obtained on supervised   learning tasks often involve an unsupervised learning component, usually   in an unsupervised pre-training phase. The main question investigated   here is the following: why does unsupervised pre-training work so well?   Through extensive experimentation, we explore several possible   explanations discussed in the literature including its action as a   regularizer (Erhan et al. 2009) and as an aid to optimization   (Bengio et al. 2007).  Our results build on the work of   Erhan et al. 2009, showing that unsupervised pre-training appears to   play predominantly a regularization role in subsequent supervised   training. However our results in an online setting, with a virtually unlimited   data stream, point to a somewhat more nuanced interpretation of the roles   of optimization and regularization in the unsupervised pre-training   effect.}
}

@misc{model_complexity,
      title={Model Complexity of Deep Learning: A Survey}, 
      author={Xia Hu and Lingyang Chu and Jian Pei and Weiqing Liu and Jiang Bian},
      year={2021},
      eprint={2103.05127},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}